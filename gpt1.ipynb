{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0222826b-615d-4cd7-9e43-480fe635a136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "BLOCK_SIZE = 64\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 3e-4\n",
    "MAX_ITERS = 3000\n",
    "EVAL_ITERS = 100\n",
    "N_EMBD = 384\n",
    "N_LAYER = 8\n",
    "N_HEAD = 8\n",
    "DROPOUT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48473445-2721-4abc-9664-15dfa71be75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "# 81 unique chars in the text\n",
    "chars = ''\n",
    "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "    chars = sorted(list(set(text)))\n",
    "print(chars)\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ff29437-cb7d-4ad0-8fc4-14c36082580d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61, 58, 65, 65, 68]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "# Tokenization is the process of breaking a text into smaller units called tokens (token = char here)\n",
    "# Many ML models work with numerical data\n",
    "string_to_int = { ch : i for i, ch in enumerate(chars) }\n",
    "int_to_string = { i : ch for i, ch in enumerate(chars) }\n",
    "\n",
    "# lambda s = function that takes input s and converts every char to int\n",
    "encode = lambda s : [string_to_int[c] for c in s]\n",
    "# l is a list of int\n",
    "decode = lambda l : ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "print(encode('hello'))\n",
    "print(decode([61, 58, 65, 65, 68]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0f5724-5ab5-4d55-9c16-4a2e96141090",
   "metadata": {},
   "source": [
    "A tensor is a multi-dimensional array used in PyTorch for efficient numerical computation, similar to arrays in NumPy. \n",
    "Tensors are designed to work with GPUs through CUDA and ML and DL functionalities like computing gradients.\n",
    "\n",
    "torch.long represents a 64-bit signed integer (from -2^63 to 2^63-1) and is typically used for indicing even though\n",
    "int may be sufficient for the 81 chars here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b731a56-2882-4777-8ae5-4857b0246dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([80,  1,  1, 28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,\n",
      "         1, 47, 33, 50, 25, 42, 28,  1, 33, 38,  1, 39, 50,  0,  0,  1,  1, 26,\n",
      "        49,  0,  0,  1,  1, 36, 11,  1, 30, 42, 25, 38, 35,  1, 26, 25, 45, 37,\n",
      "         0,  0,  1,  1, 25, 45, 44, 32, 39, 42,  1, 39, 30,  1, 44, 32, 29,  1,\n",
      "        47, 33, 50, 25, 42, 28,  1, 39, 30,  1, 39, 50,  9,  1, 44, 32, 29,  1,\n",
      "        36, 25, 38, 28,  1, 39, 30,  1, 39, 50])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae24486-43e1-47f0-a7f2-76afe5fae714",
   "metadata": {},
   "source": [
    "A bigram language model is a type of statistical language model that predicts the likelihood of a word based on the previous word in a sequence.\n",
    "Ex: \"the cat sat,\" the bigrams are: (\"the\", \"cat\") and (\"cat\", \"sat\").\n",
    "The model will predict \"cat\" after \"the\" based on frequency and \"sat\" after \"cat\" based on frequency.\n",
    "\n",
    "Block Size: Refers to how many tokens (e.g., words or characters) are used together as input for a model to process or predict.\n",
    "\n",
    "Example: If you're training a language model with a block size of 3, you'd train it on sequences like:\n",
    "\"the cat sat\" â†’ [the, cat, sat]\n",
    "\n",
    "During training, the model essentially learns the likelihood or probability of the third word (target) for any given pair of preceding words (context). It builds a kind of mapping like: P('sat' | 'the', 'cat')\n",
    "\n",
    "After the model is trained, you can provide it with any two consecutive words, and it will predict the third word based on what it has learned.\n",
    "Input: ['cat', 'sat']\n",
    "Prediction: The model will predict 'on'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c861c3e1-ecf8-43a9-a2f0-3e373cecbf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into 80:20\n",
    "# No need to shuffle data because it's sequential\n",
    "split_point = int(0.8 * len(data))\n",
    "train_data = data[ : split_point]\n",
    "validate_data = data[split_point : ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d92f50-639d-4219-82f7-5c685a1ae83b",
   "metadata": {},
   "source": [
    "The get_batch function:\n",
    "1. Randomly selects BATCH_SIZE number of starting positions in the wizard of oz text (ex. ch. 8, 1, 2, 9)\n",
    "2. For each starting position, get the block of BLOCK_SIZE with int of the chars from the starting position (ex. [1, 58, ...])\n",
    "3. Store x and y in GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddea5a6f-ac05-4c1c-82f0-de3b35bad758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:  tensor([[67, 57,  1,  ..., 68, 76,  1],\n",
      "        [67,  1, 61,  ..., 60, 61, 73],\n",
      "        [ 1, 73, 61,  ..., 55, 58, 59],\n",
      "        ...,\n",
      "        [56, 54, 74,  ..., 54, 60, 62],\n",
      "        [71, 57,  1,  ..., 72, 73,  1],\n",
      "        [57,  1, 54,  ...,  1, 70, 74]], device='cuda:0')\n",
      "targets:  tensor([[57,  1, 72,  ..., 76,  1, 72],\n",
      "        [ 1, 61, 62,  ..., 61, 73,  1],\n",
      "        [73, 61, 58,  ..., 58, 59, 68],\n",
      "        ...,\n",
      "        [54, 74, 72,  ..., 60, 62, 56],\n",
      "        [57,  1, 73,  ..., 73,  1, 55],\n",
      "        [ 1, 54, 73,  ..., 70, 74, 62]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def get_batch(data_type):\n",
    "    data = train_data if data_type == 'train' else validate_data\n",
    "    # Batch size number of random int between 0 and len(text) - block size\n",
    "    starting_position = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))\n",
    "    # print(starting_position)\n",
    "\n",
    "    # Stack blocks/sequences in batches\n",
    "    x = torch.stack([data[i : i + BLOCK_SIZE] for i in starting_position])\n",
    "    y = torch.stack([data[i + 1 : i + BLOCK_SIZE + 1] for i in starting_position])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs: ', x)\n",
    "print('targets: ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab31a2a1-5fe8-4ccc-8b2a-b30374bbb586",
   "metadata": {},
   "source": [
    "@torch.no_grad() - no gradient to improve performance\n",
    "Not needed; just reporting loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bad44a19-b406-4128-a447-3197d355b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    # Puts model in evaluate mode (like dropout which drops out random neurons for more noise)\n",
    "    model.eval()\n",
    "    for split in ['train', 'validate']:\n",
    "        losses = torch.zeros(EVAL_ITERS)\n",
    "        for k in range(EVAL_ITERS):\n",
    "            X, y = get_batch(split)\n",
    "            logits, loss = model(X, y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    # Puts model in train mode (like weights)\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3324569c-40b9-4616-b93a-d63bb1040714",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(N_EMBD, head_size, bias=False)\n",
    "        self.query = nn.Linear(N_EMBD, head_size, bias=False)\n",
    "        self.value = nn.Linear(N_EMBD, head_size, bias=False)\n",
    "        # No look ahead masking\n",
    "        # self.register_buffer('tril', torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(128, 128)))\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input size (batch, time-step, channels)\n",
    "        B, T, C = x.shape\n",
    "        key = self.key(x)   # (B, T, head size)\n",
    "        query = self.query(x)   # (B, T, head size)\n",
    "\n",
    "        # Compute attention scores ('affinities')\n",
    "        weight = query @ key.transpose(-2, -1) * key.shape[-1] ** -0.5   # (B, T, head size) @ (B, head size, T) -> (B, T, T)\n",
    "\n",
    "        weight = weight.masked_fill(self.tril[:T, :T] == 0, float('-inf'))   # (B, T, T), turn 0 to -inf\n",
    "        # weight = weight.masked_fill(\n",
    "        #     torch.tril(torch.ones(T, T, device=x.device)) == 0, float('-inf')\n",
    "        # )\n",
    "\n",
    "        weight = F.softmax(weight, dim = -1)   # (B, T, T)\n",
    "        weight = self.dropout(weight)\n",
    "\n",
    "        # Perform the weighted aggregation of the values\n",
    "        value = self.value(x)   # (B, T, head size)\n",
    "        out = weight @ value   # (B, T, T) @ (B, T, head size) -> (B, T, head_size)\n",
    "        \n",
    "        # Output size (batch, time-step, head size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c256dce8-8d00-4c63-b7f4-67c0ac50d02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        # Have heads run in parallel\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.projection = nn.Linear(head_size * num_heads, N_EMBD)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # (B, T, F) -> (B, T, [h1, h1, h1, h1, h2, h2, ...h3]) where h = head\n",
    "        out = self.dropout(self.projection(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c188bae-f354-423a-84e9-bb86f1116c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" A simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),   # <= 0 becomes 0 and > 0 stays as positive\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(DROPOUT)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3441db21-9b52-49a7-914a-e1e7fd7b9b0a",
   "metadata": {},
   "source": [
    "head_size = number of features that each head will be capturing in multi-head attention\n",
    "Layer norms help smooth out features.\n",
    "Post norm converges better for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c893f9c0-5bd9-40c6-a688-161a98e02179",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.self_attention = MultiHeadAttention(n_head, head_size)\n",
    "        self.feed_fwd = FeedForward(n_embd)\n",
    "        self.layer_norm1 = nn.LayerNorm(n_embd)\n",
    "        self.layer_norm2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.self_attention(x)\n",
    "        x = self.layer_norm1(x + y)   # Add a norm\n",
    "        y = self.feed_fwd(x)\n",
    "        x = self.layer_norm2(x + y)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666d5e39-ce57-4f12-a85b-09515d156771",
   "metadata": {},
   "source": [
    "def __init__ part:\n",
    "    - the super() calls the constructor of the parent class, nn.Module, to initialize the Bigram model as a NN model\n",
    "    - embeddings table is a lookup table: \n",
    "        Token\tIndex\tEmbedding Vector\n",
    "         'a'\t  0\t    [0.1, -0.4, 0.7, 0.3, 0.2]\n",
    "        The raw scores (logits) are learned during training.\n",
    "        They are initially random and then updated to minimize the prediction error (loss).\n",
    "        Backpropagation and optimization are used to adjust these raw scores based on how well the model predicts the next token.\n",
    "        The logits are not probabilities but raw predictions, which are converted into probabilities using softmax.\n",
    "        \n",
    "        Ex. 0.1 is the model's raw score (logit) for 'a' being the next token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af36aa0-07f8-4105-9147-493cfcd72654",
   "metadata": {},
   "source": [
    "def forward: \n",
    "    - Index = the batch (batch_size * block_size)\n",
    "    - Logits = predictions for the next char (raw scores over the vocab) = 1 raw score for each vocab for each token in the input\n",
    "               shape = batch_size * block_size * vocab_size\n",
    "    - batch_size = number of sequences in a batch (number of [ ] processed in parallel/at the same time with GPU)\n",
    "    - block_size = length of each sequence (number of tokens in a block, [ ] = 1 block)\n",
    "    - Channels = size of the output vector/neurons for each token = vocab_size\n",
    "    - Cross-Entropy Loss/Loss = measures how 'wrong' the prediction is (only used during training); compares model predictions (logits) to\n",
    "                              the correct next tokens (targets); want to minimize loss\n",
    "\n",
    "    In the else statement:\n",
    "    - Unpack the logits tensor to get the batch size, block size, and vocabulary size (4, 8, 81)\n",
    "    - Flatten the logits (32, 81) and targets (32,) to use for cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6062f675-d2f9-44b5-9e0f-3dc884ff3186",
   "metadata": {},
   "source": [
    "Transformer architecture is a deep learning model designed for sequence-to-sequence tasks, such as language translation. It relies on **self-attention mechanisms** to weigh the importance of different parts of the input data, allowing it to process sequences in parallel rather than sequentially, which speeds up training. The model is composed of **encoder** and **decoder** layers, each containing multi-head attention and feedforward networks.\n",
    "\n",
    "**Multi-head attention** is a mechanism that allows the model to focus on different parts of the input sequence simultaneously. It splits the attention into multiple \"heads,\" each learning a different representation of the data, and then combines them to form a richer understanding of the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6badc13c-4a97-4955-a79b-e25f6a7a5a14",
   "metadata": {},
   "source": [
    "def generate:\n",
    "    - Get the logits (raw scores) from self.forward for the next token prediction\n",
    "    - Change logits shape from (batch_size, block_size, vocab_size) to (batch_size, vocab_size)\n",
    "    - Converts raw logits into probabilities using softmax, ensuring they sum to 1\n",
    "    - Samples a token from the probability instead of picking the max value (adds randomness) with shape (batch_size, 1) (one sampled token per batch)\n",
    "        - dim = -1 means last dimension\n",
    "    - Append new tokens to the end of the sequences (so index shape (batch_size, block_size) becomes (batch_size, block_size + 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6004ddb-0c7f-4cda-8efb-c932bab5ae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, N_EMBD)\n",
    "        self.position_embedding_table = nn.Embedding(BLOCK_SIZE, N_EMBD)\n",
    "        # Make 4 sequential blocks\n",
    "        self.decoder_blocks = nn.Sequential(*[Block(N_EMBD, n_head=N_HEAD) for _ in range(N_LAYER)])\n",
    "\n",
    "        # For normalizing the decoders in order to feed into Softmax\n",
    "        self.final_layer_norm = nn.LayerNorm(N_EMBD)\n",
    "        self.lang_model_head = nn.Linear(N_EMBD, vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    # Apply initialization on the weights\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        # logits = self.token_embedding_table(index)\n",
    "        batch_size, block_size = index.shape\n",
    "\n",
    "        # index and targets are both (B, T) tensor of int. Torch broadcasting rules\n",
    "        token_embedding = self.token_embedding_table(index)   # (B, T, C)\n",
    "        pos_embedding = self.position_embedding_table(torch.arange(block_size, device=device))   # (T, C)\n",
    "        x = token_embedding + pos_embedding  # (B, T, C)\n",
    "        x = self.decoder_blocks(x)   # (B, T, C)\n",
    "        x = self.final_layer_norm(x)   # (B, T, C) \n",
    "        logits = self.lang_model_head(x)   # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, block_size, channels = logits.shape   # B, T, C\n",
    "            logits = logits.view(batch_size * block_size, channels)\n",
    "            targets = targets.view(batch_size * block_size)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(index)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim = -1)\n",
    "            index_next = torch.multinomial(probs, num_samples = 1)\n",
    "            index = torch.cat((index, index_next), dim = 1)\n",
    "        return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4558b567-e166-46f6-b9d7-7df89b270803",
   "metadata": {},
   "source": [
    "Initialize a starting token (0) (The context is initialized to give the model a starting point for generating new tokens)\n",
    "Generate 500 new tokens from the model.\n",
    "Convert the generated token sequence into a human-readable format.\n",
    "Right now it's untrained (random weights) so nonsense generated_chars are produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed128067-9ba4-4106-a393-db9306e44ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTLanguageModel(vocab_size)\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15281fed-c33c-4e80-bffe-2fd418e8604a",
   "metadata": {},
   "source": [
    "The AdamW optimizer includes weight decay for regularization and updates the model's parameters (weights) during training.\n",
    "The code trains the model by:\n",
    "\n",
    "1. Getting a batch of data.\n",
    "2. Performing a forward pass to compute predictions and loss.\n",
    "3. Clear the gradient to start fresh in the next iter b/c previous gradient is from previous data which can be biased. The optimizer adjusts each weight in the opposite direction of the gradient to reduce the loss (gradient descent).\n",
    "4. Calculating gradients using backpropagation.\n",
    "5. Updating the model parameters (weights) for the next iteration.\n",
    "6. Get the final loss for the last batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edfc33b6-e29d-41a3-a747-37afd23bdd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 4.425, val loss: 4.427\n",
      "step: 100, train loss: 2.321, val loss: 2.393\n",
      "step: 200, train loss: 1.859, val loss: 1.982\n",
      "step: 300, train loss: 1.614, val loss: 1.775\n",
      "step: 400, train loss: 1.466, val loss: 1.666\n",
      "step: 500, train loss: 1.363, val loss: 1.605\n",
      "step: 600, train loss: 1.280, val loss: 1.556\n",
      "step: 700, train loss: 1.224, val loss: 1.530\n",
      "step: 800, train loss: 1.162, val loss: 1.514\n",
      "step: 900, train loss: 1.110, val loss: 1.505\n",
      "step: 1000, train loss: 1.063, val loss: 1.505\n",
      "step: 1100, train loss: 1.010, val loss: 1.515\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(xb, yb)\n\u001b[0;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 12\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal loss: \u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32m~\\Desktop\\LLM\\cuda\\lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\LLM\\cuda\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\LLM\\cuda\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for iter in range(MAX_ITERS):\n",
    "    if iter % EVAL_ITERS == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['validate']:.3f}\")\n",
    "        \n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print('Final loss: ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "808044bd-7bbe-4c04-8fb9-82e793523cb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m----> 2\u001b[0m generated_chars \u001b[38;5;241m=\u001b[39m decode(\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_chars)\n",
      "Cell \u001b[1;32mIn[12], line 49\u001b[0m, in \u001b[0;36mGPTLanguageModel.generate\u001b[1;34m(self, index, max_new_tokens)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, index, max_new_tokens):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_tokens):\n\u001b[1;32m---> 49\u001b[0m         logits, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m         logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m     51\u001b[0m         probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 33\u001b[0m, in \u001b[0;36mGPTLanguageModel.forward\u001b[1;34m(self, index, targets)\u001b[0m\n\u001b[0;32m     31\u001b[0m pos_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table(torch\u001b[38;5;241m.\u001b[39marange(block_size, device\u001b[38;5;241m=\u001b[39mdevice))   \u001b[38;5;66;03m# (T, C)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m token_embedding \u001b[38;5;241m+\u001b[39m pos_embedding  \u001b[38;5;66;03m# (B, T, C)\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# (B, T, C)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm(x)   \u001b[38;5;66;03m# (B, T, C) \u001b[39;00m\n\u001b[0;32m     35\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlang_model_head(x)   \u001b[38;5;66;03m# (B, T, vocab_size)\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\LLM\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Desktop\\LLM\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\Desktop\\LLM\\cuda\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\Desktop\\LLM\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Desktop\\LLM\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[11], line 14\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 14\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm1(x \u001b[38;5;241m+\u001b[39m y)   \u001b[38;5;66;03m# Add a norm\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_fwd(x)\n",
      "File \u001b[1;32m~\\Desktop\\LLM\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Desktop\\LLM\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[9], line 12\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 12\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([h(x) \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, T, F) -> (B, T, [h1, h1, h1, h1, h2, h2, ...h3]) where h = head\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection(out))\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "Cell \u001b[1;32mIn[9], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 12\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, T, F) -> (B, T, [h1, h1, h1, h1, h2, h2, ...h3]) where h = head\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection(out))\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\Desktop\\LLM\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Desktop\\LLM\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[8], line 32\u001b[0m, in \u001b[0;36mHead.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(weight)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Perform the weighted aggregation of the values\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# (B, T, head size)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m out \u001b[38;5;241m=\u001b[39m weight \u001b[38;5;241m@\u001b[39m value   \u001b[38;5;66;03m# (B, T, T) @ (B, T, head size) -> (B, T, head_size)\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Output size (batch, time-step, head size)\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\LLM\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Desktop\\LLM\\cuda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\Desktop\\LLM\\cuda\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04a2312-ff70-4fc4-8532-416ba142ee55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
